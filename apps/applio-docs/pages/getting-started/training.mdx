import { Callout, Steps, FileTree } from "nextra/components";


# Training

Training is the process by which the program will be able to clone any voice or sound via Self-Supervised Speech.

---

<Callout type="warning" emoji="⚠️">
  - Training is only for **NVDIA GPUs**, if this is not available
  Training tab will be disabled.
  - In case the training tab is available, training is only recommended in **Nvidia Series 2000 (RTX)** or **higher**.
  - If you don’t have an compatible GPU, we offer alternatives to [run Applio in the cloud.](/getting-started/other-alternatives)
</Callout>

<Steps>
## Training a Model
 ### Step 1: Dataset Preparation
 Before you can start the training process you need to have an audio set of the desired voice, it is recommended to have
- A minimum amount of 10 minutes of clean audio, without noise or silences.
- Uncompressed audio format, this can be either `.wav` or `.flac`

The recommended duration to have a good dataset is 30 minutes, but if you have even less audio than recommended, pretraineds will be your solution to get a good model with low data.

Upload your dataset using the **Dataset Maker (if it is a single audio)** or setup it manually (various audios) to `applio/assets/datasets` creating inside a folder for the program to read it.

- [Don't know how to create a dataset?, check the dataset section](/guides/create-datasets/create-datasets)

#### For multispeaker models (Optional)
If you want to train a multispeaker model, you need to create a folder for the model, and inside that folder create a folder for each speaker.

- Speaker folders should be named **with numbers starting from 0**

- It is recommended that each speaker has a similar amount of audio time

<div style={{ display: 'flex', justifyContent: 'center', alignItems: 'center' }}>
  <FileTree>
    <FileTree.Folder name="dataset" defaultOpen>
      <FileTree.Folder name="0" defaultOpen>
        <FileTree.File name="speaker0-audio1.flac" />
        <FileTree.File name="speaker0-audio2.flac" />
        <FileTree.File name="speaker0-audio3.flac" />
      </FileTree.Folder>
      <FileTree.Folder name="1" defaultOpen>
        <FileTree.File name="speaker1-audio1.flac" />
        <FileTree.File name="speaker1-audio2.flac" />
        <FileTree.File name="speaker1-audio3.flac" />
      </FileTree.Folder>
      <FileTree.Folder name="2" defaultOpen>
        <FileTree.File name="speaker2-audio1.flac" />
        <FileTree.File name="speaker2-audio2.flac" />
        <FileTree.File name="speaker2-audio3.flac" />
      </FileTree.Folder>
    </FileTree.Folder>
  </FileTree>
</div>

### Step 2: Dataset Processing

Once you have the audio ready, give your model a name and be sure to select the correct frequency for your file `(32k, 40k, 48k)`, before running the "Preprocess dataset", consider the following options:

- **CPU cores:** The default setting are your cpu cores, which is recommended for most cases.
- **Audio cutting:** It's recommended to deactivate this option if your dataset has already been processed.
- **Process effects:** It's recommended to deactivate this option if your dataset has already been processed.

### Step 3: Feature Extraction
Now you are in the second last step!, but... what is ***feature extraction***?

Extracting features is an essential step to train, this process will convert each audio fragment divided by the post-processing step to a file readable by the F0 (Fundamental frequency).

Several F0 models are available to choose from, but the best performer is **RMVPE**, besides **CREPE (MANGIO-CREPE)** is good too but it requires very clean audio and you need a hop length of 128 or lower.
- **CPU cores:** adjust it to use in the index extraction process. The default setting are your cpu cores, which is recommended for most cases.
- **Embedder Model:** Select the model used for learning speaker embedding.
- **Pitch Guidance:** it becomes feasible to mirror the intonation of the original voice, including its pitch. This feature is particularly valuable for singing and other scenarios where preserving the original melody or pitch pattern is essential.
- **GPU Number:** Specify the number of GPUs you wish to utilize for extracting by entering them separated by hyphens (-).

When you select your model, press **Extract Features** to start the process, remember to check your CMD until you see a message indicating that the process is complete.

- [Don't know how to check sample rate?, check the Audio Analizer section](/getting-started/audio-analizer)

### Final Step: Model & Index Training 

This is where the real work begins, to start training your model you will need to make a few small adjustments before you begin.

![](/Training-Example.png)

- **Save Every Epoch:** Set this value between 10 and 50 to determine how often the model's state is saved during training.

- **Total Epochs:** The number of epochs needed varies based on your dataset. Monitor progress using TensorBoard; typically, models perform well around 200-400 epochs.

- **Batch Size:** Adjust based on your GPU's VRAM. For 8 GB VRAM, use a batch size between 6 and 8. Consider CUDA cores when experimenting with higher batch sizes.

**Index Generation:** generating an index is a must, just click on the “train index” button to perform the process.

Your trained model is located in the `logs/model folder`, and the .pth files are in the `logs/zips` folder. Also you can export your trained model directly from the Applio interface, go to the **Export Model** section in the **train** tab, click on the **Refresh** button and select the **pth and the added index** of the model to export.

<FAQBox title="Advanced Settings">
- **Pitch Guidance:** Gives variation of pitch.
- **Pretrained:** Uses the RVC pretrained, used for training common models, uncheck if you want to make a pretrain. Learn how to use it [here](/getting-started/pretrained).
- **Save Only Latest:** Replace the same D/G file newer data. This help to prevent filling up storage.
- **Save Every Weights:** Save the weights of the model when a cycle of 'Save Every Epoch' is completed.
- **Custom Pretrained:** Uses the Custom Pretrained that are loaded.
- **GPU Settings:** Allows to choose GPUs (only for users who have more than one GPU).
- **Overtraining Detector:** Mark it only if you will train for more than 200 epochs.
- **Overtraining Threshold:** Set the maximum number of epochs you want your model to stop training if no improvement is detected.
- **Sync Graph:** Synchronize the graph of the tensorbaord. Only enable this setting if you are training a new model.
- **Cache Dataset in GPU:** Cache the dataset in GPU memory to speed up the training process.
- **Embedder Model:** Select the Embedder model (contentvec, japanese-hubert-base, chinese-hubert-large or custom).
- **Index Algorithm:** KMeans is a clustering algorithm that divides the dataset into K clusters. This setting is particularly useful for large datasets, the other option are auto or Faiss.

</FAQBox>
<Callout type="info" emoji="ℹ️">
You can select the **Precision** you want to use for training and inference (fp16 or fp32) in Applio Settings tab.
</Callout>

- [Don't know how to use Tensorboard for correct training?, check out the Tensorboard section](/getting-started/tensorboard)
- [Don't know how to use custom Pretrained like TITAN and Ov2?, check out the Pretrained section](/getting-started/pretrained)

</Steps>
<Steps>
## (Optional) Resume Training
- Open Applio if you have closed it.
- Then, in the Applio interface, input your model name, use the same sample rate, and proceed to the last part of the train tab. Set the same [batch size](/faq#terminology), [pretrained](/getting-started/pretrained#where-to-find-pretraineds) (if you used) and increase the number of epochs you want to train.
- Once configured, press 'Start training' to start the process, everything is registered in the CMD.
</Steps>

export function FAQBox({ title, children }) {
  return (
    <details
      close
      className="last-of-type:mb-0 rounded-lg bg-neutral-50 dark:bg-neutral-800 p-2 mt-4"
    >
      <summary>
        <strong className="text-lg">{title}</strong>
      </summary>
      <div className="nx-p-2">{children}</div>
    </details>
  )
}
