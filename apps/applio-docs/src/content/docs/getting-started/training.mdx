---
title: Training
description: Learn how to train voice models using Self-Supervised Speech technology
---

import { Aside, Steps } from '@astrojs/starlight/components';
import { FileTree } from '@astrojs/starlight/components';

<Aside type="caution">
If you don't have a compatible GPU, we offer alternatives to [run Applio in the cloud](/getting-started/other-alternatives/).
</Aside>

## Training a Voice Model

Training is the process by which Applio learns to clone any voice or sound using Self-Supervised Speech technology. This guide will walk you through the complete training process.

### Step 1: Dataset Preparation

Before you can start training, you need to prepare an audio dataset of the desired voice. Here are the requirements:

- **Minimum duration**: 10 minutes of clean audio without noise or silences
- **Recommended duration**: 30 minutes for optimal results
- **Audio format**: Uncompressed audio files (`.wav` or `.flac`)

<Aside type="tip">
If you have less audio than recommended, pretrained models can help you achieve good results with limited data.
</Aside>

Upload your dataset using the **Dataset Maker** (for single audio files) or set it up manually (for multiple audio files) in the `applio/assets/datasets` directory. Create a folder for your dataset so the program can read it properly.

- [Don't know how to create a dataset? Check our dataset guide](/guides/how-create-datasets/)

#### Multi-Speaker Models (Optional)

If you want to train a multi-speaker model, create a folder for the model and inside that folder, create a separate folder for each speaker.

Speaker folders must be named with numbers starting from 0.

**Recommendation**: Each speaker should have a similar amount of audio data.

<FileTree>
- Dataset
  - 0
    - speaker0-audio1.flac
    - speaker0-audio2.flac
    - speaker0-audio3.flac
  - 1
    - speaker1-audio1.flac
    - speaker1-audio2.flac
    - speaker1-audio3.flac
  - 2
    - speaker2-audio1.flac
    - speaker2-audio2.flac
    - speaker2-audio3.flac
</FileTree>

### Step 2: Dataset Processing

Once your audio is ready, give your model a name and select the correct frequency for your files (`32k`, `40k`, or `48k`). Before running "Preprocess Dataset", consider these options:

- **CPU Cores**: The default setting uses all your CPU cores, which is recommended for most cases.
- **Audio Cutting**: Recommended to disable this option if your dataset has already been processed.
- **Process Effects**: Recommended to disable this option if your dataset has already been processed.

### Step 3: Feature Extraction

Feature extraction is an essential step in the training process. This process converts each audio fragment (divided by the post-processing step) into files readable by the F0 (Fundamental frequency) system.

Several F0 models are available, but **RMVPE** performs best. **CREPE (MANGIO-CREPE)** is also good but requires very clean audio and a hop length of 128 or lower.

**Configuration Options:**
- **CPU Cores**: Adjust for the index extraction process. Default uses all CPU cores.
- **Embedder Model**: Select the model used for learning speaker embedding.
- **Pitch Guidance**: Enables mirroring the intonation of the original voice, including its pitch. Particularly valuable for singing and scenarios where preserving the original melody is essential.
- **GPU Number**: Specify the number of GPUs to utilize for extraction (separate multiple GPUs with hyphens).

When you've selected your model, press **Extract Features** to start the process. Monitor your command prompt until you see a completion message.

- [Don't know how to check sample rate? Check the Audio Analyzer section](/getting-started/audio-analyzer/)

### Step 4: Model & Index Training

This is where the actual training begins. You'll need to configure a few settings before starting:

![Training Example](/images/training.png)

**Training Configuration:**
- **Save Every Epoch**: Set this value between 10 and 50 to determine how often the model's state is saved during training.
- **Total Epochs**: The number of epochs varies based on your dataset. Monitor progress using TensorBoard; typically, models perform well around 200-400 epochs.
- **Batch Size**: Adjust based on your GPU's VRAM. For 8 GB VRAM, use a batch size between 6 and 8. Consider CUDA cores when experimenting with higher batch sizes.

**Index Generation**: Generating an index is essential. Click the "Train Index" button to perform this process.

### Step 5: Export Your Model

Your trained model is located in the `logs/model` folder, and the `.pth` files are in the `logs/zips` folder. You can also export your trained model directly from the Applio interface:

<Steps>
1. Go to the **Export Model** section in the **Train** tab
2. Click the **Refresh** button
3. Select the **pth file and the corresponding index** of your model
4. Export your model
</Steps>

<Aside type="note">
You can also use a pretrained model to improve results. Learn how to use them [here](/getting-started/pretrained/).
</Aside>